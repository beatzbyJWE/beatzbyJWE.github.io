{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/project/ds5559/Group1_Netflix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the formatted data as Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df=spark.read.csv(\"formatted_data.csv\",header=True,inferSchema=True)\n",
    "movies_df=spark.read.csv(\"movie_titles.csv\",header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "movies_df=movies_df.select(col('_c0').alias(\"movie_id\"), \\\n",
    "  col('_c1').alias(\"movie_year\").cast(IntegerType()), \\\n",
    "  col('_c2').alias(\"movie_name\")) ##renaming so that we can run a simpler \"join on (name)\" later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Null Movies and recompute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4388, 4794, 7241, 10782, 15918, 16678, 17667]\n"
     ]
    }
   ],
   "source": [
    "null_movie_ids=movies_df.filter((col(\"movie_year\").isNull()) | (col(\"movie_name\") == \"NULL\")).select(\"movie_id\").rdd.map(lambda row: row[0]).collect()\n",
    "print(null_movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df=movies_df.filter(~movies_df.movie_id.isin(null_movie_ids))\n",
    "reviews_df=reviews_df.filter(~reviews_df.movie_id.isin(null_movie_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 314\n",
    "weights = [.0001, .0001,.9998]\n",
    "train, test, waste = reviews_df.randomSplit(weights, seed)\n",
    "waste = None #free up mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agg_df = reviews_df.groupBy(\"user_id\").agg(F.count(\"rating\")).sort(F.desc(\"count(rating)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_reviewers = 1000 #number of top reviewers to use, corresponds to num columns\n",
    "valid_reviewers = user_agg_df.limit(num_top_reviewers).select(\"user_id\").rdd.map(lambda row: int(row[0])).collect()\n",
    "valid_reviews = reviews_df.filter(reviews_df.user_id.isin(valid_reviewers))\n",
    "#valid_reviews.count() #uncomment to see how many reviews total we have\n",
    "train= valid_reviews #TODO maybe split this up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_train=train.limit(5)\n",
    "new_train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we do several tasks: \n",
    "* Join datasets to get year and movie id as features.\n",
    "* Create a new feature 'movie_age'\n",
    "* Drop columns that aren't going to add any information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_movies = train.join(movies_df, on=['movie_id'], how='outer') #joining frames\n",
    "reviews_movies = movies_df.join(new_train, on=['movie_id'])\n",
    "reviews_movies = reviews_movies.withColumn('year_trunc', expr(\"substring(date,0,4)\"))\n",
    "reviews_movies = reviews_movies.withColumn('movie_age',reviews_movies.year_trunc.cast(IntegerType())-reviews_movies.movie_year.cast(IntegerType())) #defining a new feature by calculating (very roughly) the age of the movie when it was reviewed\n",
    "df_for_kmeans=reviews_movies.drop(col(\"date\")).drop(col(\"year_trunc\")).drop(col(\"movie_name\")) #dropping the columns used in the calculation of movie_age\n",
    "#df_for_kmeans.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7201"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Free up some memory by killing unneeded dataframes\n",
    "reviews_movies = None\n",
    "movies_df = None\n",
    "reviews_df = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.pivotMaxValues', u'100000')\n",
    "pivoted=df_for_kmeans.groupBy(\"movie_id\").pivot(\"user_id\").avg(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = pivoted.fillna(0) #this fill na can go in the above line, left separate to test memory reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try K-Means clustering on all of the data. We include the response variable in features, because the initial goal of this unsupervised learning method is just to find out how many clusters yield the strongest silhouette score and then discuss how that score holds up. But dropping features from the model may provide some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler1 = VectorAssembler(inputCols=[col for col in pivoted.columns if col!= \"movie_id\"], outputCol=\"features\",handleInvalid = \"skip\") #assembling the features\n",
    "dataset=assembler1.transform(pivoted) #adding features using transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.select(\"movie_id\",\"features\").cache() #reduce what we need to store and cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Scaled DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|movie_id|            features|        standardized|\n",
      "+--------+--------------------+--------------------+\n",
      "|     148|(1000,[0,1,2,4,6,...|(1000,[0,1,2,4,6,...|\n",
      "|     463|(1000,[1,2,9,20,2...|(1000,[1,2,9,20,2...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale=scale.fit(dataset)\n",
    "data_scale_output=data_scale.transform(dataset)\n",
    "data_scale_output.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_range(lower,upper,dat,features): #function for iterating through various Ks and computing silhouette scores\n",
    "    scores=[]\n",
    "    ks= [k for k in range(lower,upper+1)]\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    for k in ks: #upper and lower bounds are the range; +1 is to include 10\n",
    "        kmeans = KMeans(featuresCol=features).setK(k).setSeed(314).setMaxIter(3) #k is the iterator, could probably pass \"features\" in as an argument to the function\n",
    "        model = kmeans.fit(dat) #run the model\n",
    "        predictions = model.transform(dat) #predict (in cluster or out)\n",
    "        # Evaluate clustering by computing Silhouette score\n",
    "        silhouette = evaluator.evaluate(predictions)\n",
    "        scores.append(silhouette)\n",
    "        print(k)#give progress update\n",
    "    df=pd.DataFrame({'k': pd.Series(ks, dtype='int'),\n",
    "               'sil_score': pd.Series(scores, dtype='float')}).sort_values('sil_score',ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_range_tune(lower,upper,dat,features): #function for iterating through various Ks and computing silhouette scores\n",
    "    iter_vals = [1,3,5,10,20,30,40,50,100] #configure this to test different values\n",
    "    k_vals= [k for k in range(lower,upper+1)]\n",
    "    scores=[]\n",
    "    ks = []\n",
    "    iters = []\n",
    "    \n",
    "    evaluator = ClusteringEvaluator()\n",
    "    for num_iter in iter_vals:\n",
    "        for k in k_vals: #upper and lower bounds are the range; +1 is to include 10\n",
    "            kmeans = KMeans(featuresCol= features).setK(k).setSeed(314).setMaxIter(num_iter) #k is the iterator, could probably pass \"features\" in as an argument to the function\n",
    "            model = kmeans.fit(dat) #run the model\n",
    "            predictions = model.transform(dat) #predict (in cluster or out)\n",
    "            # Evaluate clustering by computing Silhouette score\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            ks.append(k)\n",
    "            iters.append(num_iter)\n",
    "            scores.append(silhouette)\n",
    "        print(num_iter)#give progress update\n",
    "    df=pd.DataFrame({'max_iter': pd.Series(iters, dtype='int'),\n",
    "                     'k': pd.Series(ks, dtype='int'),\n",
    "               'sil_score': pd.Series(scores, dtype='float')}).sort_values('sil_score',ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "    k  sil_score\n",
      "0   2   0.686687\n",
      "1   3   0.579240\n",
      "2   4   0.479950\n",
      "3   5   0.402497\n",
      "4   6   0.253708\n",
      "6   8   0.229335\n",
      "5   7   0.148384\n",
      "7   9   0.007956\n",
      "8  10  -0.041877\n"
     ]
    }
   ],
   "source": [
    "kmeans_frame=kmeans_range(2,10,data_scale_output, 'features') #print a dataframe with the silhouette scores - this is trying on the data before scaling - do we need to use standard scaler?\n",
    "print(kmeans_frame.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running $K$ Means with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "    k  sil_score\n",
      "0   2   0.686687\n",
      "1   3   0.579240\n",
      "2   4   0.479950\n",
      "3   5   0.402497\n",
      "4   6   0.253708\n",
      "6   8   0.229335\n",
      "5   7   0.148384\n",
      "7   9   0.007956\n",
      "8  10  -0.041877\n"
     ]
    }
   ],
   "source": [
    "kmeans_frame_std=kmeans_range(2,10,data_scale_output, 'standardized') #print a dataframe with the silhouette scores - this is trying on the data before scaling - do we need to use standard scaler?\n",
    "print(kmeans_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "100\n",
      "    max_iter  k  sil_score\n",
      "0          1  2   0.705261\n",
      "9          3  2   0.690397\n",
      "18         5  2   0.686225\n",
      "54        40  2   0.685270\n",
      "36        20  2   0.685270\n",
      "45        30  2   0.685270\n",
      "27        10  2   0.685270\n",
      "72       100  2   0.685270\n",
      "63        50  2   0.685270\n",
      "38        20  4   0.556725\n"
     ]
    }
   ],
   "source": [
    "kmeans_tuned_frame=kmeans_range_tune(2,10,data_scale_output,'standardized') #print a dataframe with the silhouette scores\n",
    "print(kmeans_tuned_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|movie_id|        standardized|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|     148|(1000,[0,1,2,4,6,...|         2|\n",
      "|     463|(1000,[1,2,9,20,2...|         0|\n",
      "|     496|(1000,[116,157,20...|         1|\n",
      "|     833|(1000,[0,4,5,6,7,...|         2|\n",
      "|    1342|(1000,[116,136,15...|         1|\n",
      "|    1580|(1000,[31,70,94,1...|         1|\n",
      "|    1645|(1000,[0,3,5,6,7,...|         2|\n",
      "|    1959|(1000,[0,5,6,9,10...|         3|\n",
      "|    2122|[2.78362675444102...|         2|\n",
      "|    2366|(1000,[1,3,5,7,8,...|         0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(featuresCol='standardized').setK(4).setSeed(314).setMaxIter(3)\n",
    "model = kmeans.fit(data_scale_output) #run the model\n",
    "predictions = model.transform(data_scale_output)\n",
    "predictions.select(\"movie_id\",\"standardized\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory for Pipeline\n",
    "predictions = None\n",
    "kmeans = None\n",
    "model = None\n",
    "assembler1 = None\n",
    "dataset = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add steps into a model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[col for col in pivoted.columns if col!= \"movie_id\"], outputCol=\"features\",handleInvalid = \"skip\") #assembling the features\n",
    "\n",
    "scale = StandardScaler(inputCol='features',outputCol='standardized')\n",
    " \n",
    "kmeans = KMeans(featuresCol='standardized').setK(4).setSeed(314).setMaxIter(20)\n",
    "\n",
    "# Build the pipeline; this takes the objects as inputs\n",
    "pipeline = Pipeline(stages=[assembler, scale, kmeans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(pivoted).transform(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSave = pipeline.fit(pivoted)\n",
    "modelSave.write().overwrite().save('kmeans_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20K\tkmeans_model/metadata\n",
      "56K\tkmeans_model/stages/2_KMeans_ec58aaca6b4d/data\n",
      "20K\tkmeans_model/stages/2_KMeans_ec58aaca6b4d/metadata\n",
      "80K\tkmeans_model/stages/2_KMeans_ec58aaca6b4d\n",
      "32K\tkmeans_model/stages/0_VectorAssembler_71bc2cda2c83/metadata\n",
      "36K\tkmeans_model/stages/0_VectorAssembler_71bc2cda2c83\n",
      "40K\tkmeans_model/stages/1_StandardScaler_7da191d12dfe/data\n",
      "20K\tkmeans_model/stages/1_StandardScaler_7da191d12dfe/metadata\n",
      "64K\tkmeans_model/stages/1_StandardScaler_7da191d12dfe\n",
      "184K\tkmeans_model/stages\n",
      "208K\tkmeans_model\n"
     ]
    }
   ],
   "source": [
    "!du -h kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|movie_id|        standardized|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|     148|(1000,[0,1,2,4,6,...|         0|\n",
      "|     463|(1000,[1,2,9,20,2...|         0|\n",
      "|     496|(1000,[116,157,20...|         1|\n",
      "|     833|(1000,[0,4,5,6,7,...|         0|\n",
      "|    1342|(1000,[116,136,15...|         1|\n",
      "|    1580|(1000,[31,70,94,1...|         1|\n",
      "|    1645|(1000,[0,3,5,6,7,...|         0|\n",
      "|    1959|(1000,[0,5,6,9,10...|         3|\n",
      "|    2122|[2.78362675444102...|         2|\n",
      "|    2366|(1000,[1,3,5,7,8,...|         0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.select('movie_id','standardized','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+----------+\n",
      "|movie_id|prediction|user_id|rating|      date|\n",
      "+--------+----------+-------+------+----------+\n",
      "|     148|         0|1907667|   3.0|2005-12-15|\n",
      "|     148|         0|1744889|   1.0|2002-09-04|\n",
      "|     148|         0|1294425|   4.0|2004-04-08|\n",
      "|     148|         0|1174811|   3.0|2002-06-14|\n",
      "|     148|         0| 461110|   4.0|2003-10-20|\n",
      "|     148|         0| 838130|   4.0|2003-05-14|\n",
      "|     148|         0| 327122|   1.0|2003-07-07|\n",
      "|     148|         0| 603277|   1.0|2001-12-24|\n",
      "|     148|         0|  76196|   1.0|2001-11-29|\n",
      "|     148|         0|2161899|   2.0|2002-06-26|\n",
      "|     148|         0| 844049|   3.0|2004-06-07|\n",
      "|     148|         0|2503887|   3.0|2004-08-04|\n",
      "|     148|         0| 908205|   3.0|2005-03-29|\n",
      "|     148|         0|1650301|   2.0|2005-06-05|\n",
      "|     148|         0|2312349|   3.0|2004-08-18|\n",
      "|     148|         0|2551641|   3.0|2003-08-01|\n",
      "|     148|         0|1984086|   4.0|2004-08-18|\n",
      "|     148|         0|   1333|   2.0|2004-02-20|\n",
      "|     148|         0|1998055|   2.0|2003-03-09|\n",
      "|     148|         0| 200255|   3.0|2003-10-10|\n",
      "+--------+----------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|prediction|        avg_rating|\n",
      "+----------+------------------+\n",
      "|         1|2.7536498049982883|\n",
      "|         3|3.2961164065225463|\n",
      "|         2|3.4588583177975467|\n",
      "|         0| 3.137841231324859|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "preds_data = model.select(\"movie_id\",\"prediction\")\n",
    "preds_joined = preds_data.join(new_train, on=['movie_id']) #joining frames\n",
    "preds_joined.show()\n",
    "preds_joined=preds_joined.withColumn('year_trunc', expr(\"substring(date,0,4)\"))\n",
    "rating_preds = preds_joined.groupBy(\"prediction\").agg(F.avg(\"rating\").alias('avg_rating'))\n",
    "rating_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/ja6mw/ds5110/DS5110-Netflix-Rating-Predictions-PySpark/KMeans.ipynb to pdf\n",
      "[NbConvertApp] Writing 57595 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 66265 bytes to /sfs/qumulo/qhome/ja6mw/ds5110/DS5110-Netflix-Rating-Predictions-PySpark/KMeans.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf /sfs/qumulo/qhome/ja6mw/ds5110/DS5110-Netflix-Rating-Predictions-PySpark/KMeans.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
